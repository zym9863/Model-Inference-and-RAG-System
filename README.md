# Qwen3-8B 4bitæ¨ç†ä¸å°ä¸Šä¸‹æ–‡RAGç³»ç»Ÿ

[English](./README-EN.md) | ä¸­æ–‡

åŸºäºQwen/Qwen3-8Bæ¨¡å‹çš„4bité‡åŒ–æ¨ç†ç³»ç»Ÿï¼Œé›†æˆChromaDBå‘é‡æ•°æ®åº“å’ŒLlamaIndexæ¡†æ¶ï¼Œå®ç°é«˜æ•ˆçš„æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ã€‚

## æŠ€æœ¯æ ˆ

- **LLMæ¨¡å‹**: Qwen/Qwen3-8B (4bité‡åŒ–)
- **åµŒå…¥æ¨¡å‹**: google/embeddinggemma-300m
- **å‘é‡æ•°æ®åº“**: ChromaDB
- **RAGæ¡†æ¶**: LlamaIndex
- **é‡åŒ–æŠ€æœ¯**: bitsandbytes

## åŠŸèƒ½ç‰¹æ€§

- ğŸš€ 4bité‡åŒ–æ¨ç†ï¼Œæ˜¾å­˜å ç”¨ä½
- ğŸ“š åŸºäºChromaDBçš„é«˜æ•ˆå‘é‡æ£€ç´¢
- ğŸ” LlamaIndexé©±åŠ¨çš„RAGç®¡é“
- ğŸ¯ å°ä¸Šä¸‹æ–‡çª—å£ä¼˜åŒ–
- âš¡ å¿«é€Ÿå“åº”å’Œé«˜ååé‡

## é¡¹ç›®ç»“æ„

```
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/          # æ¨¡å‹åŠ è½½å’Œæ¨ç†
â”‚   â”œâ”€â”€ embeddings/      # åµŒå…¥æ¨¡å‹æœåŠ¡
â”‚   â”œâ”€â”€ vectordb/        # ChromaDBå‘é‡æ•°æ®åº“
â”‚   â”œâ”€â”€ rag/             # RAGæŸ¥è¯¢å¤„ç†
â”‚   â””â”€â”€ utils/           # å·¥å…·å‡½æ•°
â”œâ”€â”€ config/              # é…ç½®æ–‡ä»¶
â”œâ”€â”€ tests/               # æµ‹è¯•ä»£ç 
â”œâ”€â”€ examples/            # ä½¿ç”¨ç¤ºä¾‹
â””â”€â”€ data/                # æ•°æ®æ–‡ä»¶
```

## ç³»ç»Ÿè¦æ±‚

- Python 3.8+
- CUDA 11.0+ (å¯é€‰ï¼Œç”¨äºGPUåŠ é€Ÿ)
- è‡³å°‘8GBå†…å­˜
- è‡³å°‘10GBç£ç›˜ç©ºé—´

## å®‰è£…

### Windowsç”¨æˆ·

1. è¿è¡Œè‡ªåŠ¨å®‰è£…è„šæœ¬ï¼š
```cmd
install.bat
```

2. æˆ–æ‰‹åŠ¨å®‰è£…ï¼š
```cmd
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
venv\Scripts\activate.bat

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### Linux/Macç”¨æˆ·

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

## å¿«é€Ÿå¼€å§‹

### 1. äº¤äº’å¼ä½¿ç”¨

```bash
# å¯åŠ¨äº¤äº’å¼RAGç³»ç»Ÿ
python main.py --mode interactive

# æˆ–ä½¿ç”¨Windowsè„šæœ¬
run_example.bat
```

### 2. ç¼–ç¨‹æ¥å£

```python
from src.rag.query_processor import RAGQueryProcessor

# åˆå§‹åŒ–RAGç³»ç»Ÿ
rag_processor = RAGQueryProcessor(
    llm_model_name="Qwen/Qwen3-8B",
    embedding_model_name="google/embeddinggemma-300m",
    use_llama_index=True
)

# æ·»åŠ æ–‡æ¡£
documents = [
    {"text": "Pythonæ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€", "metadata": {"topic": "ç¼–ç¨‹"}},
    {"text": "æœºå™¨å­¦ä¹ æ˜¯AIçš„åˆ†æ”¯", "metadata": {"topic": "AI"}}
]
rag_processor.add_documents(documents)

# æŸ¥è¯¢
result = rag_processor.query("ä»€ä¹ˆæ˜¯Pythonï¼Ÿ")
print(f"å›ç­”: {result['response']}")
```

### 3. å‘½ä»¤è¡Œä½¿ç”¨

```bash
# å•æ¬¡æŸ¥è¯¢
python main.py --mode batch --query "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"

# æ·»åŠ æ–‡æ¡£
python main.py --documents doc1.txt doc2.txt

# æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
python main.py --info

# æ¸…ç©ºæ–‡æ¡£
python main.py --clear
```

## ç¤ºä¾‹

### è¿è¡ŒåŸºæœ¬ç¤ºä¾‹
```bash
python examples/basic_usage.py
```

### è¿è¡Œæ–‡æ¡£å¤„ç†ç¤ºä¾‹
```bash
python examples/document_processing.py
```

## é…ç½®

ç³»ç»Ÿé…ç½®æ–‡ä»¶ä½äº `config/config.yaml`ï¼š

```yaml
# æ¨¡å‹é…ç½®
model:
  llm_model_name: "Qwen/Qwen3-8B"
  embedding_model_name: "google/embeddinggemma-300m"
  device: "auto"

# RAGé…ç½®
rag:
  use_llama_index: true
  max_context_length: 1500
  top_k_retrieval: 5
  similarity_threshold: 0.7

# ç”Ÿæˆé…ç½®
generation:
  max_new_tokens: 512
  temperature: 0.7
  top_p: 0.9
```

### ç¯å¢ƒå˜é‡é…ç½®

å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–é…ç½®ï¼š

```bash
set RAG_SYSTEM_LLM_MODEL=your-model-name
set RAG_SYSTEM_DEVICE=cuda
set RAG_SYSTEM_PERSIST_DIRECTORY=./custom_data
```

## æµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
python -m pytest tests/ -v

# è¿è¡Œç‰¹å®šæµ‹è¯•
python tests/test_rag_system.py
```

## æ€§èƒ½ä¼˜åŒ–

1. **GPUåŠ é€Ÿ**: è®¾ç½® `device: "cuda"` ä½¿ç”¨GPU
2. **å†…å­˜ä¼˜åŒ–**: è°ƒæ•´ `chunk_size` å’Œ `max_context_length`
3. **æ‰¹å¤„ç†**: ä½¿ç”¨ `batch_size` å‚æ•°ä¼˜åŒ–åµŒå…¥è®¡ç®—
4. **ç¼“å­˜**: è®¾ç½® `cache_dir` ç¼“å­˜æ¨¡å‹æ–‡ä»¶

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **å†…å­˜ä¸è¶³**: å‡å° `chunk_size` æˆ–ä½¿ç”¨CPUæ¨¡å¼
2. **æ¨¡å‹ä¸‹è½½æ…¢**: è®¾ç½®HuggingFaceé•œåƒæˆ–ä½¿ç”¨æœ¬åœ°æ¨¡å‹
3. **CUDAé”™è¯¯**: æ£€æŸ¥CUDAç‰ˆæœ¬å…¼å®¹æ€§

### æ—¥å¿—æŸ¥çœ‹

æ—¥å¿—æ–‡ä»¶ä½äº `logs/rag_system.log`ï¼Œå¯ä»¥æŸ¥çœ‹è¯¦ç»†çš„è¿è¡Œä¿¡æ¯ã€‚

## è®¸å¯è¯

MIT License
